#!/usr/bin/python
# -*- coding: utf-8 -*-
'''This script scrapes webpages for resource lists and links to malware

Output
	Data/*					file containing resources lists, avr and avl score, number of trojan
							detection, link address, md5 name
	Virus_Label/Trojan 		md5 key for all trojan samples
	Virus_Label/* 			md5 file with malware scanners output
	Malware_dict			Dictionary of resources for all malware
	links					list of links for malware
'''
import threading
import argparse
import shutil
import pickle
import sys
import re
import os
from fractions import Fraction
from bs4 import BeautifulSoup
import urllib3

malware_dict = {}
"""map: maps md5 scraped string to resource classes """
links = []
"""list: lists the url for each Data/file in sequential order"""

class Resource():
	"""Resource class that maps one sample of malware.

    Attributes:
        resources (str): List of lists containing malware sample resources.
			i.e. files, mutex, registries
        name (str): md5 hash of malware sample. Unique Identifier
		link (str): Description of `attr2`.
		trojan (int): Description of `attr2`.
		avl (int): Description of `attr2`.
		avr (double): Description of `attr2`.
    """
	def __init__(self):
		self.resources = []
		self.name = str
		self.link = str
		self.trojan = 0
		self.avl = 0   # label depending on threshold
		self.avr = 0.0 # Ratio of malware labels positive

class myThread(threading.Thread):
	"""Thread class for multi threaded crawling of website

    Attributes:
        threadID 	(int): Identifier for thread
		counter 	(int): Number to start loop iterations at
		r 			(int): Number of iterations for loop structure in each function
		select 		(int): Selects which function to call
	"""
	def __init__(self, thread_id, counter, num_iterations, select):
		threading.Thread.__init__(self)
		self.thread_id = thread_id
		self.counter = counter*num_iterations+1
		self.num_iterations = num_iterations
		self.select = select

	def run(self):
		if self.select == 0:
			Scrape_Name(self.counter, self.num_iterations)
		if self.select == 1:
			Scrape_Resc(self.counter, self.num_iterations)



def Scrape_Name(counter, iterations):
	"""Class methods are similar to regular functions.

        Args:
            threadName: ThreadID
            counter: The number of page to start scraping resources on
			r: The number of pages to visit for scraping resources
	"""
	for w in range(counter, counter+iterations):
		# If current page is larger than total desired terminate. Provides granularity to request,
		# not multiple of 10
		if w > pages:
			sys.exit(0)

		# Get XML for page of malware links.
		http = urllib3.PoolManager()
		index_url = "https://malwr.com/analysis/?page="+str(w)  # change to whatever your url is
		index_page = http.urlopen('GET', index_url, preload_content=False).read()
		index_soup = BeautifulSoup(index_page, 'lxml')

		href_table = index_soup.find('table', attrs={'class':'table table-striped'})
		try:
			href_rows = href_table.find_all('tr')
		except:
			print "https://malwr.com/analysis/?page="+str(w)
			print "Request timed out."
			continue

		for row in href_rows:
			md5 = ""
			cols = row.find_all('td')
			mal = Resource()
			for ele in cols:
				a = re.search(r'(?<=href=\").*(?=\"><)', str(ele))
				if a is not None:
				   links.append(a.group())
				if cols.index(ele) == 1:
					md5 = ele.text
				if cols.index(ele) == 4:
					try:
						mal.avr = Fraction(ele.text)
						mal.avl = int(ele.text.split("/")[0])
					except:
						mal.avr = Fraction(-1)
						mal.avl = -1
			malware_dict[md5] = mal

# Scrape name of resources
def Scrape_Resc(counter, iterations):
	"""Uses link list to load webpages and download resource list"""
	for q in range(counter, counter+iterations):
		if q > (len(links)-1):
			sys.exit(0)
		http = urllib3.PoolManager()
		url = "https://malwr.com"+links[q]  # change to whatever your url is
		page = http.urlopen('GET', url, preload_content=False).read()

		soup = BeautifulSoup(page, 'lxml')

		table = soup.find_all('table', attrs={'class':'table table-striped'})
		table = table[1]
		rows = table.find_all('tr')
		for row in rows:
			cols = row.find_all("td")
			'''if rows.index(row)== 2:
				print row.find('td')'''
			if rows.index(row) == 3:
				md5 = row.find('td').text
				break

		malware = malware_dict[md5]
		malware.link = "https://malwr.com"+links[q]

		table = soup.findAll('div', {"class":"well mono"})

		files = []
		regs = []
		mutex = []
		resources = [files, regs, mutex]

		k = 0
		for i in table:
			for j in i:
				try:
					resources[k].append(j.lstrip())
				except:
					pass
			k += 1

		table = soup.find_all('table', attrs={'class':'table table-striped table-bordered'})
		av_pres = 0
		for i in table:
			title = i.find('tr').find('th')
			if title.text == "Antivirus":
				av_pres = 1
				t_rows = i.find_all('tr')
				virus_strings = []
				vendor = ""
				for row in t_rows:
					cols = row.find_all('td')
					if len(cols) > 0:
						it = iter(cols)
						vendor = next(it).text
						try:
							virus_name = next(it).text
						except:
							continue
						if virus_name == "\nClean\n":
							pass
						else:
							virus_strings.append(virus_name)
				g = open("Virus_Label/" + md5, "w")
				for j in virus_strings:
					if "troj" in j.lower():
						malware.trojan = 1
						f = open("Virus_Label/Trojan.txt", "a").write(md5 + "\n")
						break
					g.write(j.lower() + "\n")

				try:
					f.close()
					g.close()
				except:
					pass
				break
		if av_pres == 0:
			g = open("Virus_Label/"+md5, "w")
			g.write("NO AV")
			g.close()

		try:
			malware.resources = resources
			malware.name = md5
			pickle.dump(malware, open("Data/file"+str(q), "wb"))
		except:
			pass

def start_link(args):
	""" Start scrapping links using thread class."""
	threading.Lock()
	threads = []
	global pages
	pages = args.pages
	if args.pages < 11:
		num_threads = args.pages
		num_iterations = 1
	else:
		num_threads = 10
		num_iterations = args.pages/11 + 1

	for i in range(1, num_threads+1):
		threads.append(myThread(i, (i-1), num_iterations, 0))
		threads[i-1].start()

	for t in threads:
		t.join()

	print len(links)

def start_resources(args):
	""" Start scrapping resource lists using thread class."""
	entries = args.pages * 50
	threading.Lock()
	threads = []
	if entries < 11:
		num_threads = entries
		num_iterations = 1
	else:
		num_threads = 10
		num_iterations = entries/10+1

	for i in range(1, num_threads+1):
		threads.append(myThread(i, (i-1), num_iterations, 1))
		threads[i-1].start()

	for t in threads:
		t.join()

if __name__ == "__main__":
	print(__doc__)
	parser = argparse.ArgumentParser(description='Process number of malware samples.')
	parser.add_argument("pages", type=int, help="Defines number to crawl", default=0, nargs='?')
	parser.add_argument("--clean", help="Clear all Data files", default=False, action="store_true")
	ARGS = parser.parse_args()

	if ARGS.clean:
		shutil.rmtree("Data/", ignore_errors=True)
		os.makedirs("Data/")
		shutil.rmtree("Virus_Label/", ignore_errors=True)
		os.makedirs("Virus_Label/")
		print("Temporary File Directories Cleaned.")

	# Disable Unverified HTTPS Request
	urllib3.disable_warnings()

	print("Begin collecting Links...")
	start_link(ARGS)
	print("Completed Collecting Links...")

	print("Begin collecting Resources...")
	start_resources(ARGS)
	print("Completed Collecting Resources...")

	print("Dumping Malware Dictionary...")
	pickle.dump(malware_dict, open("malware_dict", "wb"))
	print("Dumping Link Directory...")
	pickle.dump(links, open("links", "wb"))
