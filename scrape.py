#!/usr/bin/python
# -*- coding: utf-8 -*-
'''This script scrapes webpages for resource lists and links to malware'''
import threading, sys, argparse, re, pickle, urllib3, shutil, os
from bs4 import BeautifulSoup
from fractions import Fraction

'''
Output
	Data/*					file containing resources lists, avr and avl score, number of trojan
							detection, link address, md5 name
	Virus_Label/Trojan 		md5 key for all trojan samples
	Virus_Label/* 			md5 file with malware scanners output
	Malware_dict			Dictionary of resources for all malware
	links					list of links for malware
'''

malware_dict = {}
"""map: maps md5 scraped string to resource classes """
links = []
"""list: lists the url for each Data/file in sequential order"""

class Resource():
	"""Resource class that maps one sample of malware.

    Attributes:
        resources (str): List of lists containing malware sample resources. 
			i.e. files, mutex, registries
        name (str): md5 hash of malware sample. Unique Identifier
		link (str): Description of `attr2`.
		trojan (int): Description of `attr2`.
		avl (int): Description of `attr2`.
		avr (double): Description of `attr2`.
    """
	def __init__(self):
		self.resources = []
		self.name = str
		self.link = str
		self.trojan = 0
		self.avl = 0   # label depending on threshold
		self.avr = 0.0 # Ratio of malware labels positive

class myThread(threading.Thread):
	"""Thread class for multi threaded crawling of website

    Attributes:
        threadID 	(int): Identifier for thread
		counter 	(int): Number to start loop iterations at
		r 			(int): Number of iterations for loop structure in each function
		select 		(int): Selects which function to call
	"""
	def __init__(self, threadID, counter, r, select):
		threading.Thread.__init__(self)
		self.threadID = threadID
		self.counter = counter*r+1
		self.r = r
		self.select = select
	def run(self):
		if self.select == 0:
			Scrape_Name(self.name, self.counter, self.r)
		if self.select == 1:
			Scrape_Resc(self.name, self.counter, self.r)



def Scrape_Name(threadName, counter, r):
	"""Class methods are similar to regular functions.

        Args:
            threadName: ThreadID
            counter: The number of page to start scraping resources on
			r: The number of pages to visit for scraping resources
	"""
	for w in range(counter, counter+r):
		# If current page is larger than total desired terminate. Provides granularity to request,
		# not multiple of 10
		if w > pages:
			sys.exit(0)

		# Get XML for page of malware links.
		http = urllib3.PoolManager()
		index_url = "https://malwr.com/analysis/?page="+str(w)  # change to whatever your url is
		index_page = http.urlopen('GET', index_url, preload_content=False).read()
		index_soup = BeautifulSoup(index_page, 'lxml')

		href_table = index_soup.find('table', attrs={'class':'table table-striped'})
		try:
			href_rows = href_table.find_all('tr')
		except:
			print "https://malwr.com/analysis/?page="+str(w)
			print "Request timed out."
			continue

		for row in href_rows:
			md5 = ""
			cols = row.find_all('td')
			mal = Resource()
			for ele in cols:
				a = re.search(r'(?<=href=\").*(?=\"><)', str(ele))
				if a is not None:
				   links.append(a.group())
				if cols.index(ele) == 1:
					md5 = ele.text
				if cols.index(ele) == 4:
					try:
						mal.avr = Fraction(ele.text)
						mal.avl = int(ele.text.split("/")[0])
					except:
						mal.avr = -1
						mal.avl = -1
			malware_dict[md5] = mal

# Scrape name of resources
def Scrape_Resc(threadName, counter, r):
	for q in range(counter, counter+r):
		if q > (len(links)-1):
			sys.exit(0)
		http = urllib3.PoolManager()
		url = "https://malwr.com"+links[q]  # change to whatever your url is
		page = http.urlopen('GET', url, preload_content=False).read()

		soup = BeautifulSoup(page, 'lxml')

		table = soup.find_all('table', attrs={'class':'table table-striped'})
		table = table[1]
		rows = table.find_all('tr')
		for row in rows:
			cols = row.find_all("td")
			'''if rows.index(row)== 2:
				print row.find('td')'''
			if rows.index(row)== 3:
				md5 = row.find('td').text
				break
		
		malware = malware_dict[md5]
		malware.link = "https://malwr.com"+links[q]
		
		table = soup.findAll('div', {"class":"well mono"}) 

		files = []
		regs = []
		mutex = []
		resources = [files,regs,mutex]

		k = 0
		for i in table:
			for j in i:
				try:
					resources[k].append(j.lstrip())
				except:
					pass
			k+=1

		table = soup.find_all('table', attrs={'class':'table table-striped table-bordered'})
		av_pres = 0
		for i in table:
			title = i.find('tr').find('th')
			if title.text == "Antivirus":
				av_pres = 1
				t_rows = i.find_all('tr')
				virus_strings = []
				vendor = "temp"
				for row in t_rows:
					cols = row.find_all('td')
					if len(cols)>0:
						it = iter(cols)
						vendor = next(it).text
						try:
							virus_name = next(it).text
						except:
							continue
						if ( virus_name == "\nClean\n"):
							pass
						else:
							virus_strings.append(virus_name)
				g = open("Virus_Label/"+md5,"w")
				for j in virus_strings:
					if "troj" in j.lower():
						malware.trojan = 1
						f = open("Virus_Label/Trojan.txt","a").write(md5+"\n")
						break
					g.write(j.lower()+"\n")
				
				try:
					f.close()
					g.close()
				except:
					pass
				break
		if av_pres == 0:
			g = open("Virus_Label/"+md5,"w")
			g.write("NO AV")
			g.close()
		
		try: 
			malware.resources = resources
			malware.name = md5
			pickle.dump( malware, open( "Data/file"+str(q), "wb" ) )
		except:
			pass

def start_link():
	""" Start scrapping links using thread class."""
	threadLock = threading.Lock()
	threads = []
	global pages
	pages = args.pages
	if args.pages < 11:
		a = args.pages
		b = 1
	else:
		a = 10
		b = args.pages/11 + 1
	
	for i in range (1,a+1):
		threads.append(myThread(i,(i-1),b,0))
		threads[i-1].start()
	
	for t in threads:
		t.join()
	
	print len(links)
	
def start_resources():
	""" Start scrapping resource lists using thread class."""
	entries = args.pages * 50
	threadLock = threading.Lock()
	threads = []
	if entries < 11:
		a = entries
		b = 1
	else:
		a = 10
		b = entries/10+1
		
	for i in range (1,a+1):
		threads.append(myThread(i,(i-1),b,1))
		threads[i-1].start()

	for t in threads:
		t.join()
			
if __name__ == "__main__":
	print(__doc__)
	parser = argparse.ArgumentParser(description='Process number of pages and malware samples.')
	parser.add_argument("pages", type=int, help="Defines number of pages to crawl",default=0,nargs='?')
	parser.add_argument("--clean", help="Clear all Data and log files", default=False, action="store_true")
	args = parser.parse_args()
	
	if args.clean:
		shutil.rmtree("Data/",ignore_errors=True)
		os.makedirs("Data/")
		shutil.rmtree("Virus_Label/",ignore_errors=True)
		os.makedirs("Virus_Label/")
		print("Temporary File Directories Cleaned.")
	
	# Disable Unverified HTTPS Request
	urllib3.disable_warnings()
	
	print("Begin collecting Links...")
	start_link()
	print("Completed Collecting Links...")
	print("Begin collecting Resources...")
	start_resources()
	print("Completed Collecting Resources...")
	
	print("Dumping Malware Dictionary...")
	pickle.dump( malware_dict, open( "malware_dict", "wb" ) )
	print("Dumping Link Directory...")
	pickle.dump( links, open( "links", "wb" ) )